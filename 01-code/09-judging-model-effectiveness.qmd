---
title: "Judging Model Effectiveness"
author: "Mardan Mirzaguliyev"
format: html
editor: visual
date: 2024/10/08
---

# Judging Model Effectiveness

Once we have a model, we need to know how well it works. A quantitative approach for estimating effectiveness allows us to understand the model, to compare different models, or to tweak the model to improve performance. Our focus in `tidymodels` is on empirical validation; this usually means using data that were not used to create the model as the substrate to measure effectiveness.

When judging model effectiveness, your decision about which metrics to examine can be critical.

Choosing the wrong metric can easily result in unintended consequences. For example, two common metrics for regression models are the root mean squared error (RMSE) and the coefficient of determination (a.k.a. $R^2$). The former measures *accuracy* while the latter measures *correlation*. These are not necessarily the same thing.

**Root Mean Squared Error (RMSE)** and **Coefficient of Determination (**$R^2$) are both metrics used to evaluate how well a model fits data, particularly in regression tasks. Here’s a simple explanation of each:

1.  **Root Mean Squared Error (RMSE):**

    -   RMSE tells us how much, on average, the predictions from a model deviate from the actual values.

    -   To compute it, you:

        1.  Subtract each predicted value from the actual value to get the “errors.”

        2.  Square each of these errors to avoid negative values.

        3.  Take the average of these squared errors (this is called the **Mean Squared Error**).

        4.  Finally, take the square root of the result to bring the scale back to the original unit of measurement.

    -   In simple terms, RMSE tells you how far off, on average, your predictions are from the true values. A lower RMSE means your model’s predictions are closer to the actual values.

2.  **Coefficient of Determination (R²):**

    -   R² measures how much of the variance (spread) in the actual data is explained by the model.

    -   R² ranges from 0 to 1:

        -   **1** means the model perfectly explains all the variance in the data (perfect predictions).

        -   **0** means the model explains none of the variance (no better than just guessing the average).

    -   In simple terms, R² tells you how well your model explains the variation in the data. The closer R² is to 1, the better your model fits the data.

3.  **Quick Summary:**

    -   **RMSE** tells you how far off the predictions are, on average.

    -   **R²** tells you how much of the variation in the data your model can explain.

## 9.1 PERFORMANCE METRICS AND INFERENCE

-   The effectiveness of any given model depends on how the model will be used.

-   An inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model.

-   For a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important.

-   Predictive strength is usually determined by how close our predictions come to the observed data, i.e., fidelity of the model predictions to the actual results. This chapter focuses on functions that can be used to measure predictive strength.

-   However, our advice for those developing inferential models is to use these techniques even when the model will not be used with the primary goal of prediction.

## 9.2 REGRESSION METRICS

```{r}
#| label: laod necessary packages

library(tidymodels)
tidymodels_prefer()
```

-   Recall from Section 6.3 that `tidymodels` prediction functions produce tibbles with columns for the predicted values.

-   These columns have consistent names, and the functions in the **yardstick** package that produce performance metrics have consistent interfaces.

-   The functions are data frame-based, as opposed to vector-based, with the general syntax of:

`function(data, truth, ...)`

-   The model `lm_wflow_fit` combines a linear regression model with a predictor set supplemented with an interaction and spline functions for longitude and latitude.

-   It was created from a training set (named `ames_train`). Although we do not advise using the test set at this juncture of the modeling process, it will be used here to illustrate functionality and syntax.

-   The data frame `ames_test` consists of 588 properties. To start, let’s produce predictions:

```{r}
#| label: producing predictions

ames_test_res <- predict(lm_fit, new_data = ames_test |> 
                           select(-Sale_Price))
ames_test_res
```

The predicted numeric outcome from the regression model is named `.pred`. Let’s match the predicted values with their corresponding observed outcome values:

```{r}
#| label: matching the prediction values with the observed outcome values

ames_test_res <- bind_cols(ames_test_res, ames_test |>
                             select(Sale_Price))
ames_test_res
```

-   We see that these values mostly look close, but we don’t yet have a quantitative understanding of how the model is doing because we haven’t computed any performance metrics.

-   Note that both the predicted and observed outcomes are in log-10 units.

```{r}
#| label: ploting the data

ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) +
  # Create a diagonal line:
  geom_abline(lty = 2) +
  geom_point(alpha = 0.2) +
  labs(x = "Sale Price (log10)",
       y = "Predicted Sale Price (log10)") +
  coord_obs_pred() +
  theme_bw()
```

-   There is one low-price property that is substantially over-predicted, i.e., quite high above the dashed line.

Let’s compute the root mean squared error for this model using the `rmse()` function:

```{r}
#| label: compute rmse

rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

This shows us the standard format of the output of **yardstick**functions. Metrics for numeric outcomes usually have a value of “standard” for the `.estimator` column. Examples with different values for this column are shown in the next sections.

To compute multiple metrics at once, we can create a *metric set*. Let’s add $R^2$ and the mean absolute error:

```{r}
#| label: compute R2 and the mean absolute error

ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, 
             truth = Sale_Price, 
             estimate = .pred)
```

This tidy data format stacks the metrics vertically. The root mean squared error and mean absolute error metrics are both on the scale of the outcome (so `log10(Sale_Price)` for our example) and measure the difference between the predicted and observed values. The value for $R^2$ measures the squared correlation between the predicted and observed values, so values closer to one are better.

The **`yardstick`** package does *not* contain a function for adjusted $R^2$ This modification of the coefficient of determination is commonly used when the same data used to fit the model are used to evaluate the model. This metric is not fully supported in `tidymodels` because it is always a better approach to compute performance on a separate data set than the one used to fit the model.

## 9.3 BINARY CLASSIFICATION METRICS

To illustrate other ways to measure model performance, we will switch to a different example. The **modeldata** package (another one of the tidymodels packages) contains example predictions from a test data set with two classes (“Class1” and “Class2”):

```{r}
#| label: example data set to demonstrate different ways to measure model performance

options(scipen = 999, digits = 3)
data(two_class_example)
tibble(two_class_example)
```

-   The difference in number formatting between the RStudio console and a Quarto document code chunk comes from the default printing and formatting settings used in each environment.

    -   **RStudio Console**: The console uses a default display option that prints numbers in a more compact and human-readable format (often rounding to a few decimal places).

    -   **Quarto Document Code Chunk**: Quarto, when rendering output, aims to be more precise and will often use scientific notation for numbers, especially those that are small or very large.

-   **How to Control Number Formatting in Quarto:**

-   If you’d like the numbers in your Quarto document to appear more like how they are printed in the RStudio console (e.g., using fixed decimal places or avoiding scientific notation), you can adjust the output formatting.

-   **Option 1: Use format() or round()**

    -   You can explicitly control how numbers are printed using functions like format() or round(). For example:

    -   `tibble(data = format(3.589243e-06, scientific = FALSE, digits = 6))`

    -   Or if you want to round it:

    -   `tibble(data = round(3.589243e-06, 6))`

-   **Option 2: Set global options for the document**

    -   You can set global options to control the number of significant digits in your Quarto document. At the beginning of your Quarto document or code chunk, you can use:

    -   `options(scipen = 999) # This turns off scientific notation options(digits = 6) # This sets the number of digits to display.` This will apply to the entire document or the specific code chunk, depending on where you place it.

    The second and third columns are the predicted class probabilities for the test set while `predicted` are the discrete predictions.

    For the hard class predictions, a variety of **`yardstick`** functions are helpful:

```{r}
#| label: confusion matrix

conf_mat(two_class_example, truth = truth, estimate = predicted)
```

```{r}
#| label: accuracy

accuracy(two_class_example, truth, predicted)
```

```{r}
#| label: Matthews correlation cooeficient

mcc(two_class_example, truth, predicted)
```

```{r}
#| label: F1 metric

f_meas(two_class_example, truth, predicted)
```

```{r}
#| label: combining these three classification metrics together

classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)
```

-   The Matthews correlation coefficient and F1 score both summarize the confusion matrix, but compared to `mcc()`, which measures the quality of both positive and negative examples, the `f_meas()` metric emphasizes the positive class, i.e., the event of interest.

-   For binary classification data sets like this example, **yardstick** functions have a standard argument called `event_level` to distinguish positive and negative levels. The default (which we used in this code) is that the f*irst* level of the outcome factor is the event of interest.

-   There is some heterogeneity in R functions in this regard; some use the first level and others the second to denote the event of interest. We consider it more intuitive that the first level is the most important.

-   The second level logic is borne of encoding the outcome as 0/1 (in which case the second value is the event) and unfortunately remains in some packages.

-   However, `tidymodels` (along with many other R packages) require a categorical outcome to be encoded as a factor and, for this reason, the legacy justification for the second level as the event becomes irrelevant.

As an example where the second level is the event:

```{r}
#| label: second level as the event of interest

f_meas(two_class_example, truth, predicted, event_level = "second")

```

-   In this output, the `.estimator`value of “binary” indicates that the standard formula for binary classes will be used.

There are numerous classification metrics that use the predicted probabilities as inputs rather than the hard class predictions. For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two **yardstick** functions for this method: `roc_curve()` computes the data points that make up the ROC curve and `roc_auc()` computes the area under the curve.

The interfaces to these types of metric functions use the `...` argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:

```{r}
#| label: passing the probability column for the event of interest for two-class problems - computes the data points that make up the ROC curve 

two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve
```

```{r}
#| label: passing the probability column for the event of interest for two-class problems - computes the area under the curve.

roc_auc(two_class_example, truth, Class1)
```

```{r}
#| label: visualizing the receiver operating characteristic (ROC) curve 

autoplot(two_class_curve)
```

If the curve was close to the diagonal line, then the model’s predictions would be no better than random guessing. Since the curve is up in the top, left-hand corner, we see that our model performs well at different thresholds.

There are a number of other functions that use probability estimates, including `gain_curve()`, `lift_curve()`, and `pr_curve()`.

## 9.4 MULTICLASS CLASSIFICATION METRICS

What about data with three or more classes? To demonstrate, let’s explore a different example data set that has four classes:

```{r}
#| label: example data with four classes

data(hpc_cv)
tibble(hpc_cv)
```

As before, there are factors for the observed and predicted outcomes along with four other columns of predicted probabilities for each class.

The functions for metrics that use the discrete class predictions are identical to their binary counterparts:

```{r}
#| label: accuracy

accuracy(hpc_cv, obs, pred)
```

```{r}
#| label: Matthews correlation cooeficient

mcc(hpc_cv, obs, pred)
```

-   Note that, in these results, a “multiclass” `.estimator` is listed. Like “binary,” this indicates that the formula for outcomes with three or more class levels was used.

-   The Matthews correlation coefficient was originally designed for two classes but has been extended to cases with more class levels.

-   There are methods for taking metrics designed to handle outcomes with only two classes and extend them for outcomes with more than two classes. For example, a metric such as sensitivity measures the true positive rate which, by definition, is specific to two classes (i.e., “event” and “nonevent”). How can this metric be used in our example data?

    There are wrapper methods that can be used to apply sensitivity to our four-class outcome. These options are macro-averaging, macro-weighted averaging, and micro-averaging:

    -   Macro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.

    -   Macro-weighted averaging does the same but the average is weighted by the number of samples in each class.

    -   Micro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.

Using sensitivity as an example, the usual two-class calculation is the ratio of the number of correctly predicted events divided by the number of true events. The manual calculations for these averaging methods are:

```{r}
#| label: calculate class totals

class_totals <- 
  count(hpc_cv, obs, name = "totals") |> 
  mutate(class_wts = totals / sum(totals))

class_totals
```

```{r}
#| label: calculate the number of cells

cell_counts <- 
  hpc_cv |> 
  group_by(obs, pred) |> 
  count() |> 
  ungroup()

cell_counts
```

```{r}
#| label: compute the four sensitivities using 1-vs-all

one_versus_all <- 
  cell_counts |> 
  filter(obs == pred) |> 
  full_join(class_totals, by = "obs") |> 
  mutate(sens = n / totals)

one_versus_all
```

```{r}
#| label: three different estimates

one_versus_all |> 
  summarize(
    macro = mean(sens),
    macro_wts = weighted.mean(sens, class_wts),
    micro = sum(n) / sum(totals)
    )
```

Thankfully, there is no need to manually implement these averaging methods. Instead, **`yardstick`** functions can automatically apply these methods via the `estimator` argument:

```{r}
#| label: yardstick function to compute macro-average sensitivity

sensitivity(hpc_cv, obs, pred, estimator = "macro")
```

```{r}
#| label: yardstick function to compute macro-weighted average sensitivity

sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
```

```{r}
#| label: yardstick function to compute micro-average sensitivity

sensitivity(hpc_cv, obs, pred, estimator = "micro")
```

When dealing with probability estimates, there are some metrics with multiclass analogs. For example, Hand and Till (2001) determined a multiclass technique for ROC curves. In this case, a*ll* of the class probability columns must be given to the function:

```{r}
#| label: a multiclass technique for ROC curves

roc_auc(hpc_cv, obs, VF, F, M, L)
```

Macro-weighted averaging is also available as an option for applying this metric to a multiclass outcome:

```{r}
#| label: macro-weighted average to apply Hand Till metric to a multiclass outcome

roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
```

-   Finally, all of these performance metrics can be computed using **dplyr** groupings. Recall that these data have a column for the resampling groups.

-   Notice how we can pass a grouped data frame to the metric function to compute the metrics for each group:

```{r}
#| label: computing the performance metrics for each group

hpc_cv |> 
  group_by(Resample) |> 
  accuracy(obs, pred)
```

The groupings also translate to the `autoplot()` methods:

```{r}
#| label: visualizing the resuls:  Four 1-vs-all ROC curves for each fold

hpc_cv |> 
  group_by(Resample) |> 
  roc_curve(obs, VF, F, M, L) |> 
  autoplot()
```

This visualization shows us that the different groups all perform about the same, but that the `VF` class is predicted better than the `F` or `M` classes, since the `VF` ROC curves are more in the top-left corner. This example uses resamples as the groups, but any grouping in your data can be used. This `autoplot()` method can be a quick visualization method for model effectiveness across outcome classes and/or groups.

## 9.5 CHAPTER SUMMARY

Different metrics measure different aspects of a model fit, e.g., RMSE measures accuracy while the R2 measures correlation. Measuring model performance is important even when a given model will not be used primarily for prediction; predictive power is also important for inferential or descriptive models. Functions from the **`yardstick`** package measure the effectiveness of a model using data. The primary `tidymodels` interface uses tidyverse principles and data frames (as opposed to having vector arguments). Different metrics are appropriate for regression and classification metrics and, within these, there are sometimes different ways to estimate the statistics, such as for multiclass outcomes.
