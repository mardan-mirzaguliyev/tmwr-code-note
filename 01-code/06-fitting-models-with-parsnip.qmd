---
title: "Fitting Models with parsnip"
author: "Mardan Mirzaguliyev"
format: html
editor: visual
date: 2024/09/23
---

# Fitting Models with parsnip

-   The **`parsnip`** package, one of the R packages that are part of the **`tidymodels`** metapackage, provides a fluent and standardized interface for a variety of different models.

-   Specifically, we will focus on how to `fit()` and `predict()` directly with a **parsnip** object, which may be a good fit for some straightforward modeling problems.

## 6.1 CREATE A MODEL

-   Once the data have been encoded in a format ready for a modeling algorithm, such as a numeric matrix, they can be used in the model building process.

-   Suppose that a linear regression model was our initial choice. This is equivalent to specifying that the outcome data is numeric and that the predictors are related to the outcome in terms of simple slopes and intercepts:

$$
y_i = \beta_{0} + \beta_{1i}x_{1i} + \dots + \beta_{pi}x_{pi}
$$

A variety of methods can be used to estimate the model parameters:

-   Ordinary linear regression uses the traditional method of least squares to solve for the model parameters.

    -   In R, the **stats** package can be used for the first case. The syntax for linear regression using the function `lm()` is:

        `model <- lm(formula, data, ...)`

        where `...` symbolizes other options to pass to `lm()`. The function does *not* have an `x`/`y` interface, where we might pass in our outcome as `y` and our predictors as `x`.

-   Regularized linear regression adds a penalty to the least squares method to encourage simplicity by removing predictors and/or shrinking their coefficients towards zero. This can be executed using Bayesian or non-Bayesian techniques.

    -   To estimate with regularization a Bayesian model can be fit using the **rstanarm** package:

        `model <- stan_glm(formula, data, family = "gaussian", ...)`

        In this case, the other options passed via `...` would include arguments for the prior distributions of the parameters as well as specifics about the numerical aspects of the model. As with `lm()`, only the formula interface is available.

    -   A popular non-Bayesian approach to regularized regression is the **`glmnet`** model (Friedman, Hastie, and Tibshirani 2010). Its syntax is:

        `model <- glmnet(x = matrix, y = vector, family = "gaussian", ...)`

        In this case, the predictor data must already be formatted into a numeric matrix; there is only an `x`/`y` method and no formula method.

-   ChatGPT version:

    -   **Ordinary Linear Regression (Least Squares)**: Ordinary linear regression is like finding the best-fitting straight line through a cloud of points (data) on a graph. The goal is to make predictions by fitting a model that explains the relationship between the input variables (like age, weight) and the output (like price or score).

        **Least squares** is a method that helps us find that best line by minimizing the "error" between the actual data points and the line. The error here is the distance between each data point and the predicted value from the line. We square these distances (so all errors are positive) and then add them up — the goal is to make this sum as small as possible. That’s why it's called **"least squares"**.

    -   **Regularized Linear Regression**: Sometimes, a regular linear regression can overcomplicate things by using too many variables or giving too much importance to certain variables, which makes the model less general or flexible for new data.

        **Regularization** is like putting a penalty on overly complex models. It works by shrinking the coefficients (the numbers that the model multiplies each variable by) towards zero. This means that some variables become less important, or even get dropped altogether if they're not contributing much to the model.

        -   **Bayesian** methods of regularization do this by incorporating prior beliefs about how the coefficients should behave and then updating these beliefs based on the data.

        -   **Non-Bayesian** methods (like Ridge or Lasso regression) apply a mathematical rule that pushes the coefficients toward zero directly.

    -   In simple terms, **ordinary linear regression** fits the best line using a technique called least squares, while **regularized regression** simplifies the model by keeping only the most important variables, making the predictions more reliable for new data.

For `tidymodels`, the approach to specifying a model is intended to be more unified:

1.  *Specify the type of model based on its mathematical structure* (e.g., linear regression, random forest, KNN, etc).

2.  *Specify the engine for fitting the model.* Most often this reflects the software package that should be used, like Stan or **`glmnet`**. These are models in their own right, and **parsnip** provides consistent interfaces by using these as engines for modeling.

3.  *When required, declare the mode of the model.* The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model algorithm can only address one type of prediction outcome, such as linear regression, the mode is already set.

These specifications are built without referencing the data. For example, for the three cases we outlined:

```{r}
#| label: laod necessary packages
#| warning: false

library(tidymodels)
tidymodels_prefer()
```

```{r}
#| label: set engine for Ordinary Linear Regression (Least Squares)

linear_reg() |> set_engine("lm")
```

```{r}
#| label: set engine for Bayesian Regularized Linear Regression 

linear_reg() |> set_engine("stan")
```

```{r}
#| label: set engine for non-Bayesian Regularized Linear Regression

linear_reg() |> set_engine("glmnet")
```

-   Once the details of the model have been specified, the model estimation can be done with either the `fit()` function (to use a formula) or the `fit_xy()` function (when your data are already pre-processed).

-   The **`parsnip`** package allows the user to be indifferent to the interface of the underlying model; you can always use a formula even if the modeling package’s function only has the `x`/`y` interface.

-   The `translate()` function can provide details on how **`parsnip`** converts the user’s code to the package’s syntax:

```{r}
#| label: details of lm engine

linear_reg() |> set_engine("lm") |> translate()
```

```{r}
#| label: details of stan engine

linear_reg() |> set_engine("stan") |> translate()
```

```{r}
#| label: details of glmnet engine

linear_reg(penalty = 1) |> set_engine("glmnet") |> translate() 
```

-   Note that `missing_arg()` is just a placeholder for the data that has yet to be provided.

-   We supplied a required `penalty` argument for the `glmnet` engine. Also, for the Stan and `glmnet` engines, the `family` argument was automatically added as a default. As will be shown later in this section, this option can be changed.

Let’s walk through how to predict the sale price of houses in the Ames data as a function of only longitude and latitude:

```{r}
#| label: set engine for ames data

lm_model <- linear_reg() |> 
  set_engine("lm")
```

```{r}
#| label: fit the model using fit() function (applying formula)

lm_form_fit <- 
  lm_model |> 
  # Recall that Sale_Price has been pre-logged
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)

lm_form_fit
```

```{r}
#| label: fit the model using fit() function (x/y interface)

lm_xy_fit <- 
  lm_model |> 
  fit_xy(
    x = ames_train |> select(Longitude, Latitude),
    y = ames_train |> pull(Sale_Price)
  )

lm_xy_fit
```

```{r}
#| label: help file for understanding how `parshnip` argument names map to the original names

?rand_forest
```

```{r}
#| label: translate() function for understanding how `parshnip` argument names map to the original names

rand_forest(trees = 1000, min_n = 5) |> 
  set_engine("ranger") |> 
  set_mode("regression") |> 
  translate()
```

Modeling functions in **`parsnip`** separate model arguments into two categories:

-   *Main arguments* are more commonly used and tend to be available across engines.

-   *Engine arguments* are either specific to a particular engine or used more rarely.

```{r}
#| label: Engine-specific arguments can be specified in set_engine()

rand_forest(trees = 1000, min_n = 5) |> 
  set_engine("ranger", verbose = TRUE) |> 
  set_mode("regression")
```

## 6.2 USE THE MODEL RESULTS

```{r}
#| label: returning the quantities stored in a parsnip model object

lm_form_fit |> extract_fit_engine()
```

```{r}
#| label: printing the quanties of the fitted parsnip object

lm_form_fit |> extract_fit_engine() |> vcov()
```

```{r}
#| label: printing and/or saving the table of the parameter values, their uncertainty estimates and p-values

model_res <- 
  lm_form_fit |> 
  extract_fit_engine() |> 
  summary()

model_res
```

```{r}
#| label: access the model coefficient table via coef method

# The model cooefficient table is accesible via the `coef` method,
param_est <- coef(model_res)
class(param_est)

param_est
```

```{r}
#| label: converting the model object to a tidy structure

tidy(lm_form_fit)
```

-   The column names are standardized across models and do not contain any additional data (such as the type of statistical test).

-   The data previously contained in the row names are now in a column called `term`.

-   One important principle in the tidymodels ecosystem is that a function should return values that are *predictable, consistent,* and *unsurprising*.

## 6.3 MAKE PREDICTIONS

Another area where **parsnip** diverges from conventional R modeling functions is the format of values returned from `predict()`. For predictions, **parsnip** always conforms to the following rules:

1.  The results are always a tibble.

2.  The column names of the tibble are always predictable.

3.  There are always as many rows in the tibble as there are in the input data set.

```{r}
#| label: predictions in parsnip

ames_test_small <- ames_test |> slice(1:5)
predict(lm_form_fit, new_data = ames_test_small)
```

-   The row order of the predictions are always the same as the original data.

```{r}
#| label: applying the above three rules to merge predictions with the original data

ames_test_small |> 
  select(Sale_Price) |> 
  bind_cols(predict(lm_form_fit, new_data = ames_test_small)) |> 
  bind_cols(predict(lm_form_fit, new_data = ames_test_small, type = "pred_int"))
```

```{r}
#| label: example of third rule

tree_model <- 
  decision_tree(min_n = 2) |> 
  set_engine("rpart") |> 
  set_mode("regression")


tree_fit <- 
  tree_model |> 
  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)


ames_test_small |> 
  select(Sale_Price) |> 
  bind_cols(predict(tree_fit, ames_test_small))
```

## 6.4 PARSNIP-EXTENSION PACKAGES

The **`parsnip`** package itself contains interfaces to a number of models. However, for ease of package installation and maintenance, there are other tidymodels packages that have **parsnip** model definitions for other sets of models. The **discrim** package has model definitions for the set of classification techniques called discriminant analysis methods (such as linear or quadratic discriminant analysis). In this way, the package dependencies required for installing **parsnip** are reduced. A list of all of the models that can be used with **`parsnip`** (across different packages that are on CRAN) can be found at [https://www.tidymodels.org/find/](#0).

## 6.5 CREATING MODEL SPECIFICATIONS

```{r}
#| label: open UI for parsnip addin

parsnip_addin()
```
