---
title: "When Should You Trust Your Predictions?"
author: "Mardan Mirzaguliyev"
format: html
editor: visual
date: 2024/12/07
---

# When Should You Trust Your Predictions?

-   A predictive model can almost always produce a prediction, given input data.

-   However, in plenty of situations it is inappropriate to produce such a prediction.

-   When a new data point is well outside of the range of data used to create the model, making a prediction may be an inappropriate *extrapolation*.

-   A more qualitative example of an inappropriate prediction would be when the model is used in a completely different context.

-   The cell segmentation data used in Chapter 14 flags when human breast cancer cells can or cannot be accurately isolated inside an image.

-   A model built from these data could be inappropriately applied to stomach cells for the same purpose.

-   We can produce a prediction but it is unlikely to be applicable to the different cell type.

-   This chapter discusses two methods for quantifying the potential quality of a prediction:

    -   *Equivocal zones* use the predicted values to alert the user that results may be suspect.

    -   *Applicability* uses the predictors to measure the amount of extrapolation (if any) for new samples.

## 19.1 EQUIVOCAL RESULTS

-   In some cases, the amount of uncertainty associated with a prediction is too high to be trusted.

-   If a model result indicated that you had a 51% chance of having contracted COVID-19, it would be natural to view the diagnosis with some skepticism.

-   In fact, regulatory bodies often require many medical diagnostics to have an *equivocal zone*.

-   This zone is a range of results in which the prediction should not be reported to patients, for example, some range of COVID-19 test results that are too uncertain to be reported to a patient.

-   See Danowski et al. (1970) and Kerleguer et al. (2003) for examples.

-   The same notion can be applied to models created outside of medical diagnostics.

-   Let’s use a function that can simulate classification data with two classes and two predictors (`x` and `y`).

-   The true model is a logistic regression model with the equation:

$$
\mathrm{logit}(p) = -1 - 2x - \frac{x^2}{5} + 2y^2
$$

-   The two predictors follow a bivariate normal distribution with a correlation of 0.70. We’ll create a training set of 200 samples and a test set of 50:

```{r}
#| label: load necessary packages, set the theme

library(tidymodels)
library(ggplot2)
tidymodels_prefer()
theme_set(theme_bw())
```

```{r}
#| label: define a function to simulate classes

simulate_two_classes <- 
  function(n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2)) {
    # Slighly correlated predictors
    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
    colnames(dat) <- c("x", "y")
    cls <- paste0("class_", 1:2)
    dat <- 
      as_tibble(dat) |> 
      mutate(
        linear_pred = !!eqn,
        # Add some misclassification noise
        linear_pred = linear_pred + rnorm(n, sd = error),
        prob = binomial()$linkinv(linear_pred),
        class = ifelse(prob > runif(n), cls[1], cls[2]),
        class = factor(class, levels = cls)
      )
    dplyr::select(dat, x, y, class)
  }
```

```{r}
#| label: use the function to build the training set

training_set <- simulate_two_classes(200)
training_set
```

```{r}
#| label: use the function to build the test data set

testing_set <- simulate_two_classes(50)
testing_set
```

-   We estimate a logistic regression model using Bayesian methods (using the default Gaussian prior distributions for the parameters):

```{r}
#| label: training the model

two_class_mod <- 
  logistic_reg() |> 
  set_engine("stan", seed = 1902) |> 
  fit(class ~ . + I(x^2) + I(y^2), data = training_set)

two_class_mod
```

The fitted class boundary is overlaid onto the test set in Figure 19.1.

-   The data points closest to the class boundary are the most uncertain. If their values changed slightly, their predicted class might change.

-   One simple method for disqualifying some results is to call them “equivocal” if the values are within some range around 50% (or the appropriate probability cutoff for a certain situation).

-   Depending on the problem the model is being applied to, this might indicate we should collect another measurement or we require more information before a trustworthy prediction is possible.

![](images/Screenshot%202024-12-08%20at%2011.02.18.png){width="360"}

-   We could base the width of the band around the cutoff on how performance improves when the uncertain results are removed.

-   However, we should also estimate the reportable rate (the expected proportion of usable results).

-   For example, it would not be useful in real-world situations to have perfect performance but release predictions on only 2% of the samples passed to the model.

-   Let’s use the test set to determine the balance between improving performance and having enough reportable results.

-   The predictions are created using:

```{r}
#| label: making predictions with two class training data set

test_pred <- augment(two_class_mod, testing_set)

test_pred |> head()
```

-   With tidymodels, the **`probably`** package contains functions for equivocal zones.

-   For cases with two classes, the `make_two_class_pred()` function creates a factor-like column that has the predicted classes with an equivocal zone:

```{r}
#| label: load necessary packages

library(probably)
```

```{r}
#| label: make. predictions with equivocal zone

lvls <- levels(training_set$class)

test_pred <- 
  test_pred |> 
  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))

test_pred |> count(.pred_with_eqz)
```

-   Rows that are within $0.50\pm0.15$ are given a value of `[EQ]`.

-   The notation `[EQ]` in this example is not a factor level but an attribute of that column.

-   Since the factor levels are the same as the original data, confusion matrices and other statistics can be computed without error.

-   When using standard functions from the **`yardstick`** package, the equivocal results are converted to `NA` and are not used in the calculations that use the hard class predictions.

-   Notice the differences in these confusion matrices:

```{r}
#| label: confusion matrix for the all data

test_pred |> conf_mat(class, .pred_class)
```

```{r}
#| label: confusion matrix for reportable results only

test_pred |> conf_mat(class, .pred_with_eqz)
```

-   An `is_equivocal()` function is also available for filtering these rows from the data.

-   Does the equivocal zone help improve accuracy?

-   Let’s look at different buffer sizes, as shown in Figure 19.2:

```{r}
#| label: defining a function to change the buffer then compute performance

eq_zone_results <- function(buffer) {
  test_pred <- 
    test_pred |> 
    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
  
  acc <- test_pred |> accuracy(class, .pred_with_eqz)
  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
  tibble(axxuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}
```

```{r}
#| label: Evaluate a sequence of buffers

map(seq(0, .1, length.out = 40), eq_zone_results) |> 
  list_rbind() |> 
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value")
```

```{r}
#| label: Plot the results of the effect of equivocal zones on model performance

map(seq(0, .1, length.out = 40), eq_zone_results) |> 
  list_rbind() |> 
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") |> 
  ggplot(aes(x = buffer, y = value, lty = statistic)) +
  geom_step(linewidth = 1.2, alpha = 0.8) +
  labs(y = NULL, lty = NULL)
```

-   Figure 19.2 shows us that accuracy improves by a few percentage points but at the cost of nearly 10% of predictions being unusable!

-   The value of such a compromise depends on how the model predictions will be used.

-   This analysis focused on using the predicted class probability to disqualify points, since this is a fundamental measure of uncertainty in classification models.

-   A slightly better approach would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates we found are actually the mean of the posterior predictive distribution.

-   In other words, the Bayesian model gives us a distribution for the class probability. Measuring the standard deviation of this distribution gives us a s*tandard error of prediction* of the probability.

-   In most cases, this value is directly related to the mean class probability. You might recall that, for a Bernoulli random variable with probability $p$, the variance is $p(p-1)$.

-   Because of this relationship, the standard error is largest when the probability is 50%.

-   Instead of assigning an equivocal result using the class probability, we could instead use a cutoff on the standard error of prediction.

-   One important aspect of the standard error of prediction is that it takes into account more than just the class probability.

-   In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase.

-   The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain).

-   One reason we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this.

-   For our test set, using `type = "pred_int"` will produce upper and lower limits and the `std_error` adds a column for that quantity.

-   For 80% intervals:

```{r}
#| label: adding a column for the standard error

test_pred <- 
  test_pred |> 
  bind_cols(
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )

test_pred
```

-   For our example where the model and data are well behaved, Figure 19.3 shows the standard error of prediction across the space:

![](images/Screenshot%202024-12-08%20at%2012.18.37.png){width="282"}

-   Using the standard error as a measure to preclude samples from being predicted can also be applied to models with numeric outcomes.

-   However, as shown in the next section, this may not always work.

## 19.2 DETERMINING MODEL APPLICABILITY

-   Equivocal zones try to measure the reliability of a prediction based on the model outputs.

-   It may be that model statistics, such as the standard error of prediction, cannot measure the impact of extrapolation, and so we need another way to assess whether to trust a prediction and answer, “Is our model applicable for predicting a specific data point?”

-   Let’s take the Chicago train data used extensively in Kuhn and Johnson (2019) and first shown in Section 2.2.

-   The goal is to predict the number of customers entering the Clark and Lake train station each day.

-   The data set in the **`modeldata`** package (a tidymodels package with example data sets) has daily values between January 22, 2001 and August 28, 2016.

-   Let’s create a small test set using the last two weeks of the data:

```{r}
#| labal: loading both `Chicago` data set as well as `stations`

data("Chicago")

Chicago <- Chicago |> select(ridership, date, one_of(stations))

head(Chicago)
```

```{r}
#| label: detect the number of rows for training and test set devision

n <- nrow(Chicago)
n
```

```{r}
#| label: define training data set

Chicago_train <- Chicago |> slice(1:(n - 14))
nrow(Chicago_train)
```

```{r}
#| label: define testing data set

Chicago_test <- Chicago |> slice((n - 13):n)
nrow(Chicago_test)
```

-   The main predictors are lagged ridership data at different train stations, including Clark and Lake, as well as the date.

-   The ridership predictors are highly correlated with one another.

-   In the following recipe, the date column is expanded into several new features, and the ridership predictors are represented using partial least squares (PLS) components.

-   PLS (Geladi and Kowalski 1986), as we discussed in Section 16.5.2, is a supervised version of principal component analysis where the new features have been decorrelated but are predictive of the outcome data.

-   Using the preprocessed data, we fit a standard linear model:

```{r}
#| label: building a recipe

base_recipe <- 
  recipe(ridership ~ ., data = Chicago_train) |> 
  # Create date features
  step_date(date) |> 
  step_holiday(date, keep_original_cols = FALSE) |> 
  # Create dummy variables from factor columns
  step_dummy(all_nominal()) |> 
  # Remove any columns with a single unique value
  step_zv(all_predictors()) |> 
  step_normalize(!!!stations) |> 
  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))

base_recipe
```

```{r}
#| label: model specification and workflow

lm_spec <- 
  linear_reg() |> 
  set_engine("lm")

lm_wflow <- 
  workflow() |> 
  add_recipe(base_recipe) |> 
  add_model(lm_spec)

lm_wflow
```

```{r}
#| label: model fitting

set.seed(1902)

lm_fit <- fit(lm_wflow, data = Chicago_train)
lm_fit
```

-   How well do the data fit on the test set? We can `predict()` for the test set to find both predictions and prediction intervals:

```{r}
#| label: making predictions

res_test <- 
  predict(lm_fit, Chicago_test) |> 
  bind_cols(
    predict(lm_fit, Chicago_test, type = "pred_int"), 
    Chicago_test
  )

res_test |> select(date, ridership, starts_with(".pred"))
```

```{r}
#| label: Check Root Mean Squared Error

res_test |> rmse(ridership, .pred)
```

-   These are fairly good results. Figure 19.4 visualizes the predictions along with 95% prediction intervals.

![](images/Screenshot 2024-12-09 at 22.59.10.png){width="275"}

-   Given the scale of the ridership numbers, these results look particularly good for such a simple model.

-   If this model were deployed, how well would it have done a few years later in June 2020? The model successfully makes a prediction, as a predictive model almost always will when given input data:

```{r}
#| label: predictions for the June, 2020 data

res_2020 <- 
  predict(lm_fit, Chicago_2020) |> 
  bind_cols(
    predict(lm_fit, Chicago_2020, type = "pred_int"),
    Chicago_2020
  )

res_2020 |> select(date, contains(".pred"))
```

-   The prediction intervals are about the same width, even though these data are well beyond the time period of the original training set.

-   However, given the global pandemic in 2020, the performance on these data are abysmal:

```{r}
#| label: check the performance for the 2020 predictions

res_2020 |> select(date, ridership, starts_with(".pred"))
```

```{r}
#| label: Check Root Mean Squared Error for 2020 predictions

res_2020 |> rmse(ridership, .pred)
```

-   You can see this terrible model performance visually in Figure 19.5.

![](images/Screenshot 2024-12-09 at 23.23.44.png){width="416"}

-   Confidence and prediction intervals for linear regression expand as the data become more and more removed from the center of the training set.

-   However, that effect is not dramatic enough to flag these predictions as being poor.

-   Sometimes the statistics produced by models don’t measure the quality of predictions very well.
