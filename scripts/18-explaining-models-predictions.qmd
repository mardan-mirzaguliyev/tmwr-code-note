---
title: "Explaining Models and Predictions"
author: "Mardan Mirzaguliyev"
format: html
editor: visual
date: 2024/12/03
---

# Explaining Models and Predictions

-   In Section 1.2, we outlined a taxonomy of models and suggested that models typically are built as one or more of descriptive, inferential, or predictive.

-   We suggested that model performance, as measured by appropriate metrics (like RMSE for regression or area under the ROC curve for classification), can be important for all modeling applications.

-   Similarly, model explanations, answering *why* a model makes the predictions it does, can be important whether the purpose of your model is largely descriptive, to test a hypothesis, or to make a prediction.

-   Answering the question “why?” allows modeling practitioners to understand which features were important in predictions and even how model predictions would change under different values for the features.

-   This chapter covers how to ask a model why it makes the predictions it does.

-   For some models, like linear regression, it is usually clear how to explain why the model makes its predictions.

-   The structure of a linear model contains coefficients for each predictor that are typically straightforward to interpret.

-   For other models, like random forests that can capture nonlinear behavior by design, it is less transparent how to explain the model’s predictions from only the structure of the model itself.

-   Instead, we can apply model explainer algorithms to generate understanding of predictions.

-   There are two types of model explanations, *global* and *local*.

-   Global model explanations provide an overall understanding aggregated over a whole set of observations; local model explanations provide information about a prediction for a single observation.

## 18.1 SOFTWARE FOR MODEL EXPLANATIONS

-   The tidymodels framework does not itself contain software for model explanations. Instead, models trained and evaluated with tidymodels can be explained with other, supplementary software in R packages such as **`lime`**, **`vip`**, and `DALEX`.

-   We often choose:

-   **`vip`** functions when we want to use *model-based* methods that take advantage of model structure (and are often faster)

-   **`DALEX`** functions when we want to use *model-agnostic* methods that can be applied to any model

-   In Chapters 10 and 11, we trained and compared several models to predict the price of homes in Ames, IA, including a linear model with interactions and a random forest model, with results shown in Figure 18.1.

![](images/Screenshot 2024-12-03 at 23.23.06.png){width="365"}

-   Let’s build model-agnostic explainers for both of these models to find out why they make these predictions.

-   We can use the **`DALEXtra`** add-on package for **`DALEX`**, which provides support for `tidymodels`.

-   Biecek and Burzykowski (2021) provide a thorough exploration of how to use **`DALEX`** for model explanations; this chapter only summarizes some important approaches, specific to `tidymodels`.

-   To compute any kind of model explanation, global or local, using **DALEX**, we first prepare the appropriate data and then create an *explainer* for each model:

```{r}
#| label: load necessary packages

library(DALEXtra)
```

```{r}
#| label: define the features for model-agnostic explainer

vip_features <- 
  c("Neighborhood", "Gr_Liv_Area", "Year_Built", "Bldg_Type", 
    "Latitude", "Longitude")

vip_features
```

```{r}
#| label: load necessary packages

library(dplyr)
```

```{r}
#| label: extraxting vip features from training data

vip_train <- 
  ames_train |> 
  select(all_of(vip_features))

vip_train
```

```{r}
#| label: model-agnostic explainer: linear regression model

explainer_lm <- 
  explain_tidymodels(
    lm_fit,
    data = vip_train,
    y = ames_train$Sale_Price,
    label = "lm + interactions",
    verbose = FALSE
  )

explainer_lm
```

```{r}
#| label: model-agnostic explainer: random forest model

explainer_rf <- 
  explain_tidymodels(
    rf_fit,
    data = vip_train,
    y = ames_train$Sale_Price,
    label = "random forest",
    verbose = FALSE
  )

explainer_rf
```

-   A linear model is typically straightforward to interpret and explain; you may not often find yourself using separate model explanation algorithms for a linear model.

-   However, it can sometimes be difficult to understand or explain the predictions of even a linear model once it has splines and interaction terms!

-   Dealing with significant feature engineering transformations during model explainability highlights some options we have (or sometimes, ambiguity in such analyses).

-   We can quantify global or local model explanations either in terms of:

    -   *original, basic predictors* as they existed without significant feature engineering transformations, or

    -   *derived features*, such as those created via dimensionality reduction (Chapter 16) or interactions and spline terms, as in this example.
