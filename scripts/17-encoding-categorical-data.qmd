---
title: "Encoding Categorical Data"
author: "Mardan Mirzaguliyev"
format: html
editor: visual
date: 2024/11/29
---

# Encoding Categorical Data

-   For statistical modeling in R, the preferred representation for categorical or nominal data is a *factor*, which is a variable that can take on a limited number of different values; internally, factors are stored as a vector of integer values together with a set of text labels.

-   In Section 8.4.1 we introduced feature engineering approaches to encode or transform qualitative or nominal data into a representation better suited for most model algorithms.

-   We discussed how to transform a categorical variable, such as the `Bldg_Type` in our Ames housing data (with levels `OneFam`, m`TwoFmCon`, `Duplex`, `Twnhs`, and `TwnhsE`), to a set of dummy or indicator variables like those shown in Table 17.1.

![](images/Screenshot%202024-11-29%20at%2022.32.49.png){width="382"}

-   Many model implementations require such a transformation to a numeric representation for categorical data.

-   However, for some realistic data sets, straightforward dummy variables are not a good fit.

-   This often happens because there are *too many* categories or there are *new* categories at prediction time.

-   In this chapter, we discuss more sophisticated options for encoding categorical predictors that address these issues.

-   These options are available as tidymodels recipe steps in the `embed` and `textrecipes` packages.

## 17.1 IS AN ENCODING NECESSARY?

-   A minority of models, such as those based on trees or rules, can handle categorical data natively and do not require encoding or transformation of these kinds of features.

-   A tree-based model can natively partition a variable like `Bldg_Type` into groups of factor levels, perhaps `OneFam` alone in one group and `Duplex` and `Twnhs` together in another group.

-   Naive Bayes models are another example where the structure of the model can deal with categorical variables natively; distributions are computed within each level, for example, for all the different kinds of `Bldg_Type` in the data set.

-   These models that can handle categorical features natively can *also*deal with numeric, continuous features, making the transformation or encoding of such variables optional. Does this help in some way, perhaps with model performance or time to train models?

-   Typically no, as Section 5.7 of M. Kuhn and Johnson (2020) shows using benchmark data sets with untransformed factor variables compared with transformed dummy variables for those same features. In short, using dummy encodings did not typically result in better model performance but often required more time to train the models.

-   We advise starting with untransformed categorical variables when a model allows it; note that more complex encodings often do not result in better performance for such models.

## 17.2 ENCODING ORDINAL PREDICTORS

-   Sometimes qualitative columns can be *ordered*, such as “low,” “medium,” and “high”.

-   In base R, the default encoding strategy is to make new numeric columns that are polynomial expansions of the data.

-   For columns that have five ordinal values, like the example shown in Table 17.2, the factor column is replaced with columns for linear, quadratic, cubic, and quartic terms:

![](images/Screenshot%202024-11-30%20at%2013.31.56.png)

-   While this is not unreasonable, it is not an approach that people tend to find useful.

-   For example, an 11-degree polynomial is probably not the most effective way of encoding an ordinal factor for the months of the year.

-   Instead, consider trying recipe steps related to ordered factors, such as `step_unorder()`, to convert to regular factors, and `step_ordinalscore()`, which maps specific numeric values to each factor level.

## 17.3 USING THE OUTCOME FOR ENCODING PREDICTORS

-   There are multiple options for encodings more complex than dummy or indicator variables.

-   One method called *effect* or *likelihood encodings* replaces the original categorical variables with a single numeric column that measures the effect of those data (Micci-Barreca 2001; Zumel and Mount 2019).

-   For example, for the neighborhood predictor in the Ames housing data, we can compute the mean or median sale price for each neighborhood (as shown in Figure 17.1) and substitute these means for the original data values:

```{r}
#| label: load necessary packages

library(ggplot2)
library(dplyr)
theme_set(theme_bw())
```

```{r}
#| label: mean sales price of the neighborhoods

ames_train |> 
  group_by(Neighborhood) |> 
  summarize(mean = mean(Sale_Price),
            std_err = sd(Sale_Price) / sqrt(length(Sale_Price))) |> 
  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) +
  geom_point() +
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +
  labs(y = NULL, x = "Price (mean, log scale)")
  


```

-   This kind of effect encoding works well when your categorical variable has many levels.

-   In tidymodels, the **embed** package includes several recipe step functions for different kinds of effect encodings, such as `step_lencode_glm()`, `step_lencode_mixed()`, and `step_lencode_bayes()`.

-   These steps use a generalized linear model to estimate the effect of each level in a categorical predictor on the outcome.

-   When using a recipe step like `step_lencode_glm()`, specify the variable being encoded first and then the outcome using `vars()`:

```{r}
#| label: load necessary packages
library(tidymodels)
tidymodels_prefer()
library(embed)
```

```{r}
#| label: estimating the effect of each level in a categorical predictor on the outcome

ames_glm <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) |> 
  step_log(Gr_Liv_Area, base = 10) |> 
  # glm - Generalized Linear Model
  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_")) |> 
  step_ns(Latitude, Longitude, deg_free = 20)

ames_glm
```

-   As detailed in Section 16.4, we can `prep()` our recipe to fit or estimate parameters for the preprocessing transformations using training data.

-   We can then `tidy()` this prepared recipe to see the results:

```{r}
#| label: fitting the recipe

glm_estimates <- 
  prep(ames_glm) |> 
  tidy(number = 2)

glm_estimates
```

-   When we use the newly encoded `Neighborhood` numeric variable created via this method, we substitute the original level (such as `"North_Ames"`) with the estimate for `Sale_Price` from the GLM.

-   Effect encoding methods like this one can also seamlessly handle situations where a novel factor level is encountered in the data.

-   This `value` is the predicted price from the GLM when we don’t have any specific neighborhood information:

```{r}
#| label: handling situations where a novel factor level is encountered

glm_estimates |> 
  filter(level == "..new")
```

-   Effect encodings can be powerful but should be used with care. The effects should be computed from the training set, after data splitting.

-   This type of supervised preprocessing should be rigorously resampled to avoid overfitting (see Chapter 10).

-   When you create an effect encoding for your categorical variable, you are effectively layering a mini-model inside your actual model.

-   The possibility of overfitting with effect encodings is a representative example for why feature engineering *must* be considered part of the model process, as described in Chapter 7, and why feature engineering must be estimated together with model parameters inside resampling.

### 17.3.1 EFFECT ENCODINGS WITH PARTIAL POOLING

-   Creating an effect encoding with `step_lencode_glm()` estimates the effect separately for each factor level (in this example, neighborhood).

-   However, some of these neighborhoods have many houses in them, and some have only a few.

-   There is much more uncertainty in our measurement of price for the single training set home in the Landmark neighborhood than the 354 training set homes in North Ames.

-   We can use *partial pooling* to adjust these estimates so that levels with small sample sizes are shrunken toward the overall mean.

-   The effects for each level are modeled all at once using a mixed or hierarchical generalized linear model:

```{r}
#| label: example of partial pooling: modeling the effects for each level all at once using a mixed linear model

ames_mixed <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) |> 
  step_log(Gr_Liv_Area, base = 10) |> 
  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_")) |> 
  step_ns(Latitude, Longitude, deg_free = 20)

ames_mixed
```

```{r}
#| label: fitting and displaying the results

mixed_estimates <- 
  prep(ames_mixed) |> 
  tidy(number = 2)

mixed_estimates
```

-   New levels are then encoded at close to the same value as with the GLM:

```{r}
#| label: encoding new levels at close to the same value as with the GLM

mixed_estimates |> 
  filter(level == "..new")
```

-   You can use a fully Bayesian hierarchical model for the effects in the same way with `step_lencode_bayes()`.

-   Let’s visually compare the effects using partial pooling vs. no pooling in Figure 17.2:

```{r}
#| label: partial pooling vs. no pooling

glm_estimates |> 
  rename(`no pooling` = value) |> 
  left_join(
    mixed_estimates |> 
      rename(`partial pooling` = value), by = "level"
  ) |> 
  left_join(
    ames_train |> 
      count(Neighborhood) |> 
      mutate(level = as.character(Neighborhood))
  ) |> 
  ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +
  geom_abline(color = "gray50", lty = 2) +
  geom_point(alpha = 0.7) +
  coord_fixed()
```

-   Notice in Figure 17.2 that most estimates for neighborhood effects are about the same when we compare pooling to no pooling.

-   However, the neighborhoods with the fewest homes in them have been pulled (either up or down) toward the mean effect.

-   When we use pooling, we shrink the effect estimates toward the mean because we don’t have as much evidence about the price in those neighborhoods.

## 17.4 FEATURE HASHING
