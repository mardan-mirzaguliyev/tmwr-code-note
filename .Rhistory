#| label: confusion matrix for reportable results only
test_pred |> conf_mat(class, .pred_with_eqz)
#| label: defining a function to change the buffer then compute performance
eq_zone_results <- function(buffer) {
test_pred <-
test_pred |>
mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
acc <- test_pred |> accuracy(class, .pred_with_eqz)
rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
tibble(axxuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}
#| label: Evaluate a sequence of buffers
map(seq(0, .1, length.out = 40), eq_zone_results) |>
list_rbind() |>
pivot_longer(c(-buffer), names_to = "statistic", values_to = "value")
#| label: Plot the results of the effect of equivocal zones on model performance
map(seq(0, .1, length.out = 40), eq_zone_results) |>
list_rbind() |>
pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") |>
ggplot(aes(x = buffer, y = value, lty = statistic)) +
geom_step(linewidth = 1.2, alpha = 0.8) +
labs(y = NULL, lty = NULL)
#| label: adding a column for the standard error
test_pred <-
test_pred |>
bind_cols(
predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
)
test_pred
#| labal: loading both `Chicago` data set as well as `stations`
data("Chicago")
Chicago <- Chicago |> select(ridership, date, one_of(stations))
head(Chicago)
#| label: detect the number of rows for training and test set devision
n <- nrow(Chicago)
n
#| label: define training data set
Chicago_train <- Chicago |> slice(1:(n - 14))
nrow(Chicago_train)
#| label: define testing data set
Chicago_test <- Chicago |> slice((n - 13):n)
nrow(Chicago_test)
#| label: building a recipe
base_recipe <-
recipe(ridership ~ ., data = Chicago_train) |>
# Create date features
step_date(date) |>
step_holiday(date, keep_original_cols = FALSE) |>
# Create dummy variables from factor columns
step_dummy(all_nominal()) |>
# Remove any columns with a single unique value
step_zv(all_predictors()) |>
step_normalize(!!!stations) |>
step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))
base_recipe
#| label: model specification and workflow
lm_spec <-
linear_reg() |>
set_engine("lm")
lm_wflow <-
workflow() |>
add_recipe(base_recipe) |>
add_model(lm_spec)
lm_wflow
#| label: model fitting
set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
lm_fit
#| label: making predictions
res_test <-
predict(lm_fit, Chicago_test) |>
bind_cols(
predict(lm_fit, Chicago_test, type = "pred_int"),
Chicago_test
)
res_test |> select(date, ridership, starts_with(".pred"))
#| label: Check Root Mean Squared Error
res_test |> rmse(ridership, .pred)
#| label: predictions for the June, 2020 data
res_2020 <-
predict(lm_fit, Chicago_2020) |>
bind_cols(
predict(lm_fit, Chicago_2020, type = "pred_int"),
Chicago_2020
)
res_2020 |> select(date, contains(".pred"))
#| label: check the performance for the 2020 predictions
res_2020 |> select(date, ridership, starts_with(".pred"))
#| label: Check Root Mean Squared Error for 2020 predictions
res_2020 |> rmse(ridership, .pred)
#| label: load necessary packages, set the theme
library(tidymodels)
library(ggplot2)
tidymodels_prefer()
theme_set(theme_bw())
#| label: define a function to simulate classes
simulate_two_classes <-
function(n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2)) {
# Slighly correlated predictors
sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
colnames(dat) <- c("x", "y")
cls <- paste0("class_", 1:2)
dat <-
as_tibble(dat) |>
mutate(
linear_pred = !!eqn,
# Add some misclassification noise
linear_pred = linear_pred + rnorm(n, sd = error),
prob = binomial()$linkinv(linear_pred),
class = ifelse(prob > runif(n), cls[1], cls[2]),
class = factor(class, levels = cls)
)
dplyr::select(dat, x, y, class)
}
#| label: use the function to build the training set
training_set <- simulate_two_classes(200)
training_set
#| label: use the function to build the test data set
testing_set <- simulate_two_classes(50)
testing_set
#| label: training the model
two_class_mod <-
logistic_reg() |>
set_engine("stan", seed = 1902) |>
fit(class ~ . + I(x^2) + I(y^2), data = training_set)
two_class_mod
#| label: making predictions with two class training data set
test_pred <- augment(two_class_mod, testing_set)
test_pred |> head()
#| label: load necessary packages
library(probably)
#| label: make. predictions with equivocal zone
lvls <- levels(training_set$class)
test_pred <-
test_pred |>
mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))
test_pred |> count(.pred_with_eqz)
#| label: confusion matrix for the all data
test_pred |> conf_mat(class, .pred_class)
#| label: confusion matrix for reportable results only
test_pred |> conf_mat(class, .pred_with_eqz)
#| label: defining a function to change the buffer then compute performance
eq_zone_results <- function(buffer) {
test_pred <-
test_pred |>
mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
acc <- test_pred |> accuracy(class, .pred_with_eqz)
rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
tibble(axxuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}
#| label: Evaluate a sequence of buffers
map(seq(0, .1, length.out = 40), eq_zone_results) |>
list_rbind() |>
pivot_longer(c(-buffer), names_to = "statistic", values_to = "value")
#| label: Plot the results of the effect of equivocal zones on model performance
map(seq(0, .1, length.out = 40), eq_zone_results) |>
list_rbind() |>
pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") |>
ggplot(aes(x = buffer, y = value, lty = statistic)) +
geom_step(linewidth = 1.2, alpha = 0.8) +
labs(y = NULL, lty = NULL)
#| label: adding a column for the standard error
test_pred <-
test_pred |>
bind_cols(
predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
)
test_pred
#| label: loading both `Chicago` data set as well as `stations`
data("Chicago")
Chicago <- Chicago |> select(ridership, date, one_of(stations))
head(Chicago)
#| label: detect the number of rows for training and test set devision
n <- nrow(Chicago)
n
#| label: define training data set
Chicago_train <- Chicago |> slice(1:(n - 14))
nrow(Chicago_train)
#| label: define testing data set
Chicago_test <- Chicago |> slice((n - 13):n)
nrow(Chicago_test)
#| label: building a recipe
base_recipe <-
recipe(ridership ~ ., data = Chicago_train) |>
# Create date features
step_date(date) |>
step_holiday(date, keep_original_cols = FALSE) |>
# Create dummy variables from factor columns
step_dummy(all_nominal()) |>
# Remove any columns with a single unique value
step_zv(all_predictors()) |>
step_normalize(!!!stations) |>
step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))
base_recipe
#| label: model specification and workflow
lm_spec <-
linear_reg() |>
set_engine("lm")
lm_wflow <-
workflow() |>
add_recipe(base_recipe) |>
add_model(lm_spec)
lm_wflow
#| label: model fitting
set.seed(1902)
lm_fit <- fit(lm_wflow, data = Chicago_train)
lm_fit
#| label: making predictions
res_test <-
predict(lm_fit, Chicago_test) |>
bind_cols(
predict(lm_fit, Chicago_test, type = "pred_int"),
Chicago_test
)
res_test |> select(date, ridership, starts_with(".pred"))
#| label: Check Root Mean Squared Error
res_test |> rmse(ridership, .pred)
#| label: predictions for the June, 2020 data
res_2020 <-
predict(lm_fit, Chicago_2020) |>
bind_cols(
predict(lm_fit, Chicago_2020, type = "pred_int"),
Chicago_2020
)
res_2020 |> select(date, contains(".pred"))
#| label: check the performance for the 2020 predictions
res_2020 |> select(date, ridership, starts_with(".pred"))
#| label: Check Root Mean Squared Error for 2020 predictions
res_2020 |> rmse(ridership, .pred)
#| label: load necessary packages
library(applicable)
pca_stat <- apd_pca(~ .,
data = Chicago_train |> select(one_of(stations)),
threshold = 0.99)
pca_stat
#| label: building an applicability domain model
pca_stat <- apd_pca(~ .,
data = Chicago_train |> select(one_of(stations)),
threshold = 0.99)
pca_stat
#| label: visualizing the results
autoplot(pca_stat, distance) + lab(x = "distance")
#| label: visualizing the results
autoplot(pca_stat, distance) + labs(x = "distance")
#| label: visualizing the results
autoplot(pca_stat, distance) + labs(x = "distance")
#| label: computing the percentiles for new data: score() function has the same functionality as predict()
score(pca_stat, Chicago_test) |> select(starts_with("distance"))
#| label: computing the percentiles for 2020 data
score(pca_stat, Chicago_2020) |> select(starts_with("distance"))
#| label: workflow set of models
race_results
race_results
#| label: workflow set of models
race_results
#| label: load necessary packages
library(tidymodels)
library(stacks)
tidymodels_prefer()
#| label: building a stack for the concrete data models
concrete_stack <-
stacks() |>
add_candidates(race_results)
concrete_stack
#| label: workflow set of models
race_results
#| label: load necessary packages
library(tidymodels)
library(stacks)
tidymodels_prefer()
#| label: building a stack for the concrete data models
concrete_stack <-
stacks() |>
add_candidates(race_results)
concrete_stack
#| label: creating a metamodel using the linera regression
set.seed(2001)
ens <- blend_predictions(concrete_stack)
ens
#| label: visualization of the model ensemble
autoplot(ens)
#| label: load necessary packages
library(tidymodels)
library(stacks)
tidymodels_prefer()
theme_set(theme_bw())
#| label: visualization of the model ensemble
autoplot(ens)
#| label: evaluating the meta-learning model with larger penalties using an additional option
set.seed(2002)
ens <- blend_predictions(concrete_stack,
penalty = 10^seq(-2, -0.5, length = 20))
ens
#| label: visualization of the model ensemble with defined penalty range
autoplot(ens)
ens
#| label: workflow set of models
race_results
#| label: load necessary packages
library(tidymodels)
library(stacks)
tidymodels_prefer()
theme_set(theme_bw())
#| label: building a stack for the concrete data models
concrete_stack <-
stacks() |>
add_candidates(race_results)
concrete_stack
#| label: creating a metamodel using the linera regression
set.seed(2001)
ens <- blend_predictions(concrete_stack)
ens
#| label: visualization of the model ensemble with default penalty range
autoplot(ens)
#| label: evaluating the meta-learning model with larger penalties using an additional option
set.seed(2002)
ens <- blend_predictions(concrete_stack,
penalty = 10^seq(-2, -0.5, length = 20))
ens
#| label: visualization of the model ensemble with defined penalty range
autoplot(ens)
ens
#| label: visualizing the contributions of each model types
autoplot(ens, "weights") +
geom_text(aes(x = weight + 0.01, label = model), hjust = 0) +
theme(legend.position = "none") +
lims(x = c(-0.01, 0.8))
#| label: training and returning canditate member models
ens <- fit_members(ens)
library(r)
library(rules)
#| label: training and returning canditate member models
ens <- fit_members(ens)
ens
#| label: load necessary packages
# library for Cubist model specification
library(rules)
#| label: training and returning canditate member models
ens <- fit_members(ens)
ens
#| label: make predictions with the ensemble
reg_metrics <- metric_set(rmse, rsq)
ens_test_pred <-
predict(ens, concrete_test) |>
bind_cols(concrete_test)
ens_test_pred |>
reg_metrics(compressive_strength, .pred)
#| label: load necessary packages
library(tidymodels)
tidymodels_prefer()
#| label: load necessary packages
library(tidymodels)
tidymodels_prefer()
theme_set(theme_bw())
#| load and visualize the data
data("bioChemists", package = "pscl")
#| label: load and visualize the data
data("bioChemists", package = "pscl")
ggplot(bioChemists, aes(x = art)) +
geom_histogram(binwidth = 1, color = "white") +
labs(x = "Number of articles within 3y of graduation")
#| label: load necessary packages
library(tidymodels)
tidymodels_prefer()
theme_set(theme_bw())
#| label: load and visualize the data
data("bioChemists", package = "pscl")
ggplot(bioChemists, aes(x = art)) +
geom_histogram(binwidth = 1, color = "white") +
labs(x = "Number of articles within 3y of graduation")
#| label: fdgdfg
bioChemists |>
group_by(fem) |>
summarize(counts = sum(art), n = length(art))
head(bioChemists)
View(bioChemists)
?bioChemists
#| label: number of men and women in the data set and number of publications per gender
bioChemists |>
group_by(fem) |>
summarize(counts = sum(art), n = length(art))
#| label: Comparison of Poisson rates
poisson.test(c(930, 619, T = 3))
#| label: Comparison of Poisson rates
poisson.test(c(930, 619), T = 3))
#| label: Comparison of Poisson rates
poisson.test(c(930, 619), T = 3)
#| label: tidying the results of Poisson test
poisson.test(c(930, 619)) |>
tidy()
#| label: load necessary packages
library(infer)
#| label: specifying the difference in the mean number of articles between sexes and calculating the statistic from the data
observed <-
bioChemists |>
specify(art ~ fem) |>
calculate(stat = "diff in means", order = c("Men", "Women"))
observed
#| label: computing a confidence interval for this mean by creating the bootstrap distribution
set.seed(2101)
bootstrapped <-
bioChemists |>
specify(art ~ fem) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "diff in means", order = c("Men", "Women"))
bootstrapped
#| label: calculating the percentile interval
percentile_ci <- get_ci(bootstrapped)
percentile_ci
#| label: visualizing the analysis results
visualize(bootstrapped) +
shade_confidence_interval(endpoints = percentile_ci)
#| label: stating the type of assumption to test and shuffling the data
set.seed(2102)
permuted <-
bioChemists |>
specify(art ~ fem) |>
hypothesize(null = "independence") |>
generate(reps = 2000, type = "permute") |>
calculate(stat = "diff in means", order = c("Men", "Women"))
permuted
#| label: visualizing the analysis results which used permutation method to assess distributional assumptions
visualize(permuted) +
shade_p_value(obs_stat = observed, direction = "two-sided")
#| label: p-value for the permutation method
permuted |>
get_p_value(obs_stat = observed, direction = "two-sided")
#| label: load necessary packages
library(tidymodels)
tidymodels_prefer()
theme_set(theme_bw())
#| label: load and visualize the data
data("bioChemists", package = "pscl")
ggplot(bioChemists, aes(x = art)) +
geom_histogram(binwidth = 1, color = "white") +
labs(x = "Number of articles within 3y of graduation")
#| label: number of men and women in the data set and number of publications per gender
bioChemists |>
group_by(fem) |>
summarize(counts = sum(art), n = length(art))
#| label: Comparison of Poisson rates
poisson.test(c(930, 619), T = 3)
#| label: tidying the results of Poisson test
poisson.test(c(930, 619)) |>
tidy()
#| label: load necessary packages
library(infer)
#| label: specifying the difference in the mean number of articles between sexes and calculating the statistic from the data
observed <-
bioChemists |>
specify(art ~ fem) |>
calculate(stat = "diff in means", order = c("Men", "Women"))
observed
#| label: computing a confidence interval for this mean by creating the bootstrap distribution
set.seed(2101)
bootstrapped <-
bioChemists |>
specify(art ~ fem) |>
generate(reps = 2000, type = "bootstrap") |>
calculate(stat = "diff in means", order = c("Men", "Women"))
bootstrapped
#| label: calculating the percentile interval
percentile_ci <- get_ci(bootstrapped)
percentile_ci
#| label: visualizing the analysis results which used bootstrap method to assess distributional assumptions
visualize(bootstrapped) +
shade_confidence_interval(endpoints = percentile_ci)
#| label: stating the type of assumption to test and shuffling the data
set.seed(2102)
permuted <-
bioChemists |>
specify(art ~ fem) |>
hypothesize(null = "independence") |>
generate(reps = 2000, type = "permute") |>
calculate(stat = "diff in means", order = c("Men", "Women"))
permuted
#| label: visualizing the analysis results which used permutation method to assess distributional assumptions
visualize(permuted) +
shade_p_value(obs_stat = observed, direction = "two-sided")
#| label: p-value for the permutation method
permuted |>
get_p_value(obs_stat = observed, direction = "two-sided")
#| label: load necessary packages
library(poissonreg)
#| label: fitting the model
# default engine is 'glm'
log_lin_spec <- poisson_reg()
log_lin_fit <-
log_lin_spec |>
fit(art ~ ., data = bioChemists)
log_lin_fit
#| label: summarizing the coefficients
tidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90)
#| label: dfdlf
set.seed(2103)
glm_boot <-
reg_intervals(art ~.,
data = bioChemists,
model_fn = "glm",
family = poisson)
glm_boot
#| label: tidy the results for the likelihood ratio tests to get the p-value
log_lin_reduced <-
log_lin_spec |>
fit(art ~ ment + kid5 + fem + mar, data = bioChemists)
anova(
extract_fit_engine(log_lin_reduced),
extract_fit_engine(log_lin_fit),
test = "LRT"
) |>
tidy()
